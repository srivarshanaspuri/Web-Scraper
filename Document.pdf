Web Scraping with Python: A Step-by-Step Guide

Table of Contents:
1. Introduction
2. Setting Up the Environment
3. Creating the Project Folder
4. Writing the Python Code
5. Running the Scraper
6. Inspecting the Database
7. Summary

1. Introduction
Web scraping is a technique used to extract data from websites. This guide will walk you through the process of scraping quotes and their authors from a website and storing the data in an SQLite database.

2. Setting Up the Environment
Before you begin, ensure you have Python installed on your machine. You will need the following Python libraries:
requests (for making HTTP requests)
beautifulsoup4 (for parsing HTML)
sqlite3 (for interacting with the SQLite database, which comes with Python by default)
To install the required libraries (except sqlite3, which is built-in), run the following command in your terminal or command prompt:
In bash:
pip install requests beautifulsoup4

3. Creating the Project Folder
Create a folder for this project, for example, web_scraping_project. Inside this folder, create the following files:
scrape.py (to hold the Python code)
quotes.db (SQLite database file; it will be generated by the code)

4. Writing the Python Code
Open your scrape.py file and add the following code step by step:
Step 4.1: Import Libraries
First, import the libraries needed for scraping and database handling.
python
import requests
from bs4 import BeautifulSoup
import sqlite3
Step 4.2: Fetch the Data from the Website
Use the requests library to get the HTML content of the website.
python
# Define the URL of the website
url = "http://quotes.toscrape.com/"
# Send an HTTP GET request to fetch the content of the page
response = requests.get(url)
# Parse the content using BeautifulSoup
soup = BeautifulSoup(response.text, 'html.parser')
Step 4.3: Parse the HTML Content
Extract the quotes and their authors from the parsed HTML content.
python
# List to hold the scraped data
quotes_data = []
# Find all quote blocks on the page
quotes = soup.find_all('div', class_='quote')
# Loop through all the quotes and extract the text and author
for quote in quotes:
    text = quote.find('span', class_='text').get_text()  # Extract the quote text
    author = quote.find('small', class_='author').get_text()  # Extract the author's name
    quotes_data.append((text, author))  # Add each quote and author to the list
Step 4.4: Set Up SQLite Database
Create an SQLite database and store the quotes in a table.
python
# Connect to SQLite database (it will create the file if it doesn't exist)
conn = sqlite3.connect('quotes.db')
cursor = conn.cursor()
# Create a table to store the quotes if it doesn't exist
cursor.execute('''
CREATE TABLE IF NOT EXISTS quotes (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    quote_text TEXT,
    author TEXT )
''')
Step 4.5: Insert Data into the Database
Insert the scraped quotes into the SQLite database.
python
# Insert multiple rows (quotes) into the table
cursor.executemany('''
INSERT INTO quotes (quote_text, author) VALUES (?, ?)
''', quotes_data)
# Commit the changes and close the connection
conn.commit()
conn.close()
Step 4.6: Complete Python Code (scrape.py)
Here's the final version of the scrape.py script:
python
import requests
from bs4 import BeautifulSoup
import sqlite3
# Define the URL of the websitE
url = "http://quotes.toscrape.com/"
# Send an HTTP GET request to fetch the content of the page
response = requests.get(url)
# Parse the content using BeautifulSoup
soup = BeautifulSoup(response.text, 'html.parser')
# List to hold the scraped data quotes_data = []
# Find all quote blocks on the page
quotes = soup.find_all('div', class_='quote')
# Loop through all the quotes and extract the text and author
for quote in quotes:
    text = quote.find('span', class_='text').get_text()  # Extract the quote text
    author = quote.find('small', class_='author').get_text()  # Extract the author's name
    quotes_data.append((text, author))  # Add each quote and author to the list
# Connect to SQLite database (it will create the file if it doesn't exist)
conn = sqlite3.connect('quotes.db')
cursor = conn.cursor()
# Create a table to store the quotes if it doesn't exist
cursor.execute('''
CREATE TABLE IF NOT EXISTS quotes (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    quote_text TEXT,
    author TEXT
)
''')
# Insert multiple rows (quotes) into the table
cursor.executemany('''
INSERT INTO quotes (quote_text, author) VALUES (?, ?)
''', quotes_data)
# Commit the changes and close the connection
conn.commit()
conn.close()
print("Data has been successfully scraped and stored in SQLite database.")

5. Running the Scraper
Step 5.1: Execute the Script
To run the Python script, navigate to your project directory in the terminal or command prompt and execute:
In bash:
python scrape.py
You should see the message:
Data has been successfully scraped and stored in SQLite database.
This indicates that the quotes have been successfully scraped and stored in the quotes.db database.

6. Inspecting the Database
To check if the data has been stored in the SQLite database:
Open your project folder. You will see a quotes.db file.
Open this database file using any SQLite database viewer (e.g., DB Browser for SQLite) to inspect the stored data.
Alternatively, you can query the data from the database directly using Python:
python
# Query the database to verify the data
conn = sqlite3.connect('quotes.db')
cursor = conn.cursor()
# Fetch all quotes
cursor.execute('SELECT * FROM quotes')
rows = cursor.fetchall()
# Print each row (quote and author)
for row in rows:
    print(f"ID: {row[0]}, Quote: {row[1]}, Author: {row[2]}")
# Close the connection
conn.close()

7. Summary
Here's a recap of what we've done:
1.Set up a Python project.
2.Installed necessary libraries (requests, beautifulsoup4).
3.Wrote Python code to scrape quotes and authors from a website.
4.Stored the scraped data in an SQLite database.
5.Verified the data in the database.